{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I - Instalation of libraries and listing directories of data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pyabf in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pyabf) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=3.0.7 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pyabf) (5.2.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pyabf) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (19.2)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (19.2.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (7.2.0)\n",
      "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (0.13.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.0.7->pyabf) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.1.0->pyabf) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.1.0->pyabf) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.1.0->pyabf) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.1.0->pyabf) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from packaging->pytest>=3.0.7->pyabf) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest>=3.0.7->pyabf) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/adrianalejandro/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pyabf) (41.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyabf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyabf\n",
    "import pyabf.tools.memtest \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import os.path\n",
    "from os import path\n",
    "import csv\n",
    "import pandas as pnd\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askdirectory\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__II - Functions for obtaining csv metadata.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_metadata(contents):   \n",
    "    all_header_files = []\n",
    "    \n",
    "    for filename in contents:\n",
    "        if '.txt' not in filename and '.csv' not in filename:\n",
    "            header_txt = filename + '_header.txt'\n",
    "            header = open(header_txt,'w')\n",
    "            all_header_files.append(header_txt)\n",
    "            abf = pyabf.ABF(filename)\n",
    "            header.write(abf.headerText)\n",
    "            header.close()\n",
    "    return(all_header_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_metadata(all_header_files):\n",
    "    \n",
    "    csv_files = []\n",
    "\n",
    "    for filename in all_header_files:\n",
    "        if '.csv' not in filename and '.txt' in filename:\n",
    "            header = open(filename,'r')\n",
    "            metadata_str = []\n",
    "    \n",
    "            for line in header:\n",
    "                if '=' in line:\n",
    "                    metadata_str.append(line)\n",
    "            \n",
    "            csv_filename = filename.replace('.txt','.csv')\n",
    "            metadata_csv = pnd.DataFrame(metadata_str)\n",
    "        \n",
    "            metadata_csv.columns = ['Data'] # pandas doesn't like column names to be 0 \n",
    "            metadata_csv[['Data','Values']] = metadata_csv.Data.str.split(\" = \",expand=True) # remove whitespaces too!!!\n",
    "            metadata_csv.to_csv(csv_filename, sep=',',index=False)\n",
    "            csv_files.append(csv_filename)\n",
    "        \n",
    "    return csv_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__III - Functions for calculating resistance and peak amplitude values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_values(csv,parameter):\n",
    "\n",
    "    # Obtains raw data string associated to specified parameter in metadata.\n",
    "    csv_metadata = pnd.read_csv(csv)\n",
    "    dic_metadata = csv_metadata.set_index('Data').to_dict()['Values'] # totally stolen from stack overflow and its FAST\n",
    "    raw_str = dic_metadata[parameter] \n",
    "    return raw_str\n",
    "\n",
    "def get_mV_step_interval(raw_str):\n",
    "    # assumes only a single voltage step in voltage clamp experiments\n",
    "    parsed_str = raw_str.split(', ')\n",
    "    parsed_total = (len(parsed_str))\n",
    "    for i in range(parsed_total):\n",
    "        if '0.00' not in parsed_str[i]:\n",
    "            # also stolen: this extracts string inside []\n",
    "            values = parsed_str[i][parsed_str[i].find(\"[\")+1:parsed_str[i].find(\"]\")]\n",
    "    step_points = (values.split(':'))\n",
    "    step_start, step_end = int(step_points[0]), int(step_points[1])\n",
    "    return step_start, step_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resistances(step_start, step_end, abf):\n",
    "    Rs = []\n",
    "    Ri = []\n",
    "    for i in abf.sweepList:\n",
    "        abf.setSweep(i)\n",
    "        neg_maxI = min(abf.sweepY[step_start:step_end])\n",
    "        SeriesR = (abf.sweepC[step_start + 10]/neg_maxI) * 1000 #convert from gigaohms to megaohms\n",
    "        Rs.append(SeriesR)\n",
    "        Input_I = abf.sweepY[step_end - 10]\n",
    "        InputR = ((abf.sweepC[step_end -10])/Input_I) * 1000\n",
    "        Ri.append(InputR) \n",
    "    return Ri, Rs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RiRs_persweep(csv_file,abf):\n",
    "    parameter = 'sweepEpochs' # This can be switched around if you want to use this function as a template for\n",
    "                              # functions to call other parameters\n",
    "    values = get_metadata_values(csv_file,parameter)    \n",
    "    step_start, step_end = get_mV_step_interval(values)\n",
    "    Ri, Rs = get_resistances(step_start, step_end, abf)\n",
    "    return Ri, Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_pA(abf,csv_files):\n",
    "    \n",
    "    parameter = 'dataRate' # tickrate\n",
    "    peak_pA = []\n",
    "    peak_pA_ind = []\n",
    "    peak_pA_s = []\n",
    "    latency_s = []\n",
    "    pre_window_s = 0.005 # seconds to look at after artifact to find real starting point for EPSC, 100 ticks\n",
    "    post_window_s = 0.01 # seconds to look after the real starting point to find EPSC peak, 200 ticks\n",
    "    i1, i2 = 0, 5000 # first set of wide indices\n",
    "    \n",
    "    for i in abf.sweepList:\n",
    "        \n",
    "        tickrate = float(get_metadata_values(csv_files[i],parameter))\n",
    "        abf.setSweep(i)\n",
    "        \n",
    "        pre_window_ticks = tickrate*pre_window_s \n",
    "        post_window_ticks = tickrate*post_window_s\n",
    "    \n",
    "        pos_max_ind = np.argmax(abf.sweepY[i1:i2]) # huge positive artifact\n",
    "\n",
    "        #Fake calculations to show how numbers are managed with each sum operation\n",
    "        pre_window_i1 = pos_max_ind + 2\n",
    "        pre_window_i2 = int(pre_window_i1 + pre_window_ticks)\n",
    "        # 3100 + 2 = 3102\n",
    "        # 31200 + 100 = 3202\n",
    "    \n",
    "        post_window_i1 = int(pre_window_i1 + (np.argmax(abf.sweepY[pre_window_i1:pre_window_i2])))\n",
    "        post_window_i2 = int(post_window_i1 + post_window_ticks)\n",
    "        #3102 + 25 = 3127\n",
    "        #3127 + 200 = 3327\n",
    "\n",
    "        peak = min(abf.sweepY[post_window_i1:post_window_i2])\n",
    "        peak_ind = int(post_window_i1 + (np.argmin(abf.sweepY[post_window_i1:post_window_i2])))\n",
    "        peak_s = peak_ind/tickrate\n",
    "        latency = peak_s - (pos_max_ind/tickrate) # peak time normalized to stimulus onset (huge positive current)\n",
    "        #3127 + 10 = 3137 (index of peak pA)\n",
    "        \n",
    "        peak_pA.append(peak)       \n",
    "        peak_pA_ind.append(peak_ind) \n",
    "        peak_pA_s.append(peak_s)\n",
    "        latency_s.append(latency)\n",
    "    \n",
    "    return peak_pA, peak_pA_ind, peak_pA_s, latency_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IV - Functions for generating `DataFrame` with all measures above.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prep(mouse,contents):\n",
    "    abf_files = []\n",
    "    csv_files = []\n",
    "\n",
    "    for filename in contents:\n",
    "        if '.txt' not in filename and '.csv' not in filename:\n",
    "            abf_files.append(filename)\n",
    "        elif '.csv' in filename and mouse not in filename:\n",
    "            csv_files.append(filename) \n",
    "    abf_files.sort()\n",
    "    csv_files.sort()\n",
    "    abf_files.pop(0)\n",
    "    csv_files.pop(0)\n",
    "    return abf_files, csv_files\n",
    "\n",
    "\n",
    "def get_stim_currents(mouse,contents): #stimulus currents file must be in csv format in the same folder as its abf's\n",
    "    for file in contents:\n",
    "        if mouse in file:\n",
    "            segment_file = file\n",
    "            current_steps = pnd.read_csv(segment_file)\n",
    "            stim_int = current_steps['Stim Intensity (uA)'].tolist()\n",
    "            stim_int = stim_int[1:-1] # some cleanup \n",
    "    return stim_int\n",
    "\n",
    "def data_summary_percell(abf_files,csv_files,stim_int):\n",
    "    mean_Ri = []\n",
    "    mean_Rs = []\n",
    "    mean_peak_pA = []\n",
    "    all_peak_times = pnd.DataFrame()\n",
    "    for i in range(len(abf_files)):\n",
    "        abf = pyabf.ABF(abf_files[i]) \n",
    "        Ri, Rs = get_RiRs_persweep(csv_files[i],abf)\n",
    "        # Get average resistances per file and save onto dataframe\n",
    "        mean_Ri.append(np.mean(Ri))\n",
    "        mean_Rs.append(np.mean(Rs))\n",
    "    \n",
    "        # Get peak amplitude of EPSC\n",
    "        peak_pA, peak_pA_ind, peak_pA_s, latency_s = get_peak_pA(abf,csv_files)\n",
    "        mean_peak_pA.append(np.mean(peak_pA))\n",
    "        seg = abf_files[i].strip('.abf')\n",
    "        segment = [seg for j in abf.sweepList]\n",
    "        sweep_num = abf.sweepList\n",
    "        peak_times = pnd.DataFrame(list(zip(peak_pA_ind,peak_pA_s,latency_s,segment)), index=sweep_num, columns =  \n",
    "                          [\"peak_pA_frame\", \"peak_pA_time_(s)\",\"peak_latency_(s)\",\"sweep_number\"])\n",
    "        all_peak_times = all_peak_times.append(peak_times,ignore_index=False)\n",
    "    \n",
    "    cell_df = pnd.DataFrame(list(zip(stim_int, mean_Ri, mean_Rs, mean_peak_pA)), index=abf_files, columns =  \n",
    "                          [\"stimulus_intensity_uA\", \"Input_Resistance_MOhm\", \"Series_Resistance_MOhm\",\n",
    "                           'Peak_amplitude_pA'])                       \n",
    "    pivot_peak_times = all_peak_times.pivot(index=all_peak_times.index, columns='sweep_number') # Tidy the data by sweep number :)\n",
    "    return cell_df , pivot_peak_times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master function that generates dataframes containing Rs, Rs, peak_pA and respective timepoints, for each cell\n",
    "\n",
    "def summarize_cell_VClamp(mouse,cell_dir):\n",
    "    \n",
    "    os.chdir(cell_dir) # change directory to cell abf files\n",
    "    if os.path.exists('.DS_Store') == True: # my own computer creates this given that i use a backup drive, \n",
    "        os.remove('.DS_Store')   \n",
    "        \n",
    "    contents = os.listdir('.') # list all contents in this cell folder\n",
    "    all_header_files = get_txt_metadata(contents) \n",
    "    all_csv_files = get_csv_metadata(all_header_files)\n",
    "    all_header_files.sort()\n",
    "    all_csv_files.sort()\n",
    "    abf_files, csv_files = df_prep(mouse,contents) #ignore first sweeps that are just an equilibration\n",
    "    stim_int = get_stim_currents(mouse,contents)\n",
    "    summary, all_peak_times = data_summary_percell(abf_files,csv_files,stim_int)\n",
    "    \n",
    "    return summary, all_peak_times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adrianalejandro/Desktop/Gomez_rotation/sample data/171219_SCH-028953\n",
      "['028953slice1cell1', '028953slice1cell2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n",
      "WARNING:pyabf.abf:fileGUID isn't truly unique (fileUUID is)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = askdirectory(title='Select Folder with all data from one genotype') # shows dialog box and return the path \n",
    "os.chdir(path)\n",
    "\n",
    "if os.path.exists('.DS_Store') == True: # my own computer creates this given that i use a backup drive, \n",
    "    os.remove('.DS_Store')          \n",
    "gen_dir = os.getcwd()\n",
    "mouse_folders = os.listdir('.')\n",
    "\n",
    "for mouse in mouse_folders:    \n",
    "    mouse_dir = gen_dir + '/' + mouse # get directory for one mouse\n",
    "    os.chdir(mouse_dir)  #change directory to that mouse\n",
    "    if os.path.exists('.DS_Store') == True: # my own computer creates this given that i use a backup drive, \n",
    "        os.remove('.DS_Store')    \n",
    "    cwd = os.getcwd() \n",
    "    print(cwd)\n",
    "    cell_folders = os.listdir(cwd) # list the cells (directories) of that mouse\n",
    "    print(cell_folders) \n",
    "    for cell in cell_folders:\n",
    "        \n",
    "        os.chdir(mouse_dir)\n",
    "        if cell == '.DS_Store':\n",
    "            os.remove('.DS_Store')    \n",
    "        elif 'cell' in cell and 'processed' not in cell:\n",
    "            processed_file = cell + '_processed.csv'\n",
    "            processed_dir = cwd + '/' + processed_file\n",
    "            os.mkdir(processed_dir)\n",
    "            cell_dir = mouse_dir + '/' + cell\n",
    "            summary, all_peak_times = summarize_cell_VClamp(mouse,cell_dir)\n",
    "            \n",
    "            summ_path = processed_dir + '/' + cell + '_summary.csv'\n",
    "            tp_path = processed_dir + '/'  + cell + '_timepoints.csv'\n",
    "            summary.to_csv(summ_path, index = True)\n",
    "            all_peak_times.to_csv(tp_path, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
